import json
import os
import time

import requests
from typing import Dict, List, Any

DEEPSEEK_URL = ""
API_KEY = ""
HEADERS = {"Authorization": f"Bearer {API_KEY}", "Content-Type": "application/json"}

LANG_MAP = {
    "vi": "è¶Šå—è¯­",
    "th": "æ³°è¯­",
    "zh": "ä¸­æ–‡"
}

SYSTEM_PROMPT = """æ‚¨æ˜¯å¤šè¯­è¨€å®ä½“å¯¹é½ä¸“å®¶ï¼Œè¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹è§„åˆ™éªŒè¯å®ä½“å¯¹ï¼š
1. ç¿»è¯‘éªŒè¯ï¼šæ˜¯å¦ä¸ºå®˜æ–¹ç¿»è¯‘/æ ‡å‡†éŸ³è¯‘
2. è¯­ä¹‰éªŒè¯ï¼šç±»å‹ä¸€è‡´ã€æ ¸å¿ƒå±æ€§åŒ¹é…
3. åªè¿”å›å®Œå…¨åŒ¹é…çš„å®ä½“å¯¹

è¾“å‡ºæ ¼å¼ï¼š{"matches": [["åŸå®ä½“", "equal", "ç›®æ ‡å®ä½“"]]}"""


def build_prompt(source_entity: str, entity_type: str, candidates: List[Dict], target_lang: str) -> str:
    candidate_desc = "\n".join(
        [f"{i + 1}. {c['entity']} (ç›¸ä¼¼åº¦: {c['similarity']:.4f})"
         for i, c in enumerate(candidates)]
    )
    return f"""æºå®ä½“ï¼š{source_entity}ï¼ˆç±»å‹ï¼š{entity_type}ï¼‰
ç›®æ ‡è¯­è¨€ï¼š{LANG_MAP[target_lang]}
å€™é€‰åˆ—è¡¨ï¼š
{candidate_desc}

è¯·éªŒè¯å¹¶è¾“å‡ºç¡®è®¤ä¸ºåŒä¸€å®ä½“çš„åŒ¹é…å¯¹ï¼š"""


def call_deepseek(prompt: str) -> List[List[str]]:
    try:
        response = requests.post(
            DEEPSEEK_URL,
            headers=HEADERS,
            json={
                "model": "deepseek-chat",
                "messages": [
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.1
            },
            timeout=30
        )
        if response.status_code == 200:
            result = json.loads(response.json()["choices"][0]["message"]["content"])
            return result.get("matches", [])
    except Exception as e:
        print(f"APIè°ƒç”¨å¤±è´¥: {str(e)}")
    return []


def process_entities(input_file: str, output_file: str, batch_size: int = 100):
    with open(input_file, "r", encoding="utf-8") as f:
        data = json.load(f)

    results = []
    batch_count = 0
    start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
    total_entities = len(data)
    processed_pairs = 0  # æ–°å¢å¤„ç†è®¡æ•°å™¨

    print(f"ğŸŸ¢ å¼€å§‹å¤„ç†ï¼Œå…± {total_entities} ä¸ªå®ä½“éœ€è¦å¤„ç†")

    for idx, (source_entity, entity_info) in enumerate(data.items()):
        entity_type = entity_info["type"]

        # æ–°å¢è¯­è¨€å¯¹ç»Ÿè®¡
        lang_pairs = [lp for lp in ["zh->vi", "zh->th", "vi->th"] if lp in entity_info["matches"]]
        print(f"\nğŸ”µ å¤„ç†ç¬¬ {idx + 1}/{total_entities} ä¸ªå®ä½“: {source_entity}")
        print(f"   ğŸ“Œ éœ€è¦å¤„ç†çš„è¯­è¨€å¯¹: {len(lang_pairs)} ä¸ª")

        for lang_idx, lang_pair in enumerate(lang_pairs):
            target_lang = lang_pair.split("->")[-1]
            candidates = entity_info["matches"][lang_pair]
            processed_pairs += 1  # æ›´æ–°è®¡æ•°å™¨

            # æ–°å¢å€™é€‰æ•°é‡æ˜¾ç¤º
            print(f"   ğŸŒ å¤„ç†è¯­è¨€å¯¹ ({lang_idx + 1}/{len(lang_pairs)}) {lang_pair}")
            print(f"      ğŸ“‹ å€™é€‰æ•°é‡: {len(candidates)} | å·²ç”¨æ—¶é—´: {time.time() - start_time:.1f}s")

            prompt = build_prompt(source_entity, entity_type, candidates, target_lang)
            matches = call_deepseek(prompt)

            # æ–°å¢åŒ¹é…ç»“æœç»Ÿè®¡
            if matches:
                valid_matches = [m for m in matches if m[1] == "equal"]
                print(f"      âœ… å‘ç°æœ‰æ•ˆåŒ¹é…: {len(valid_matches)} æ¡")
                results.extend(valid_matches)
            else:
                print("      âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆåŒ¹é…")

            # æ–°å¢è¿›åº¦é¢„æµ‹
            avg_time = (time.time() - start_time) / processed_pairs if processed_pairs else 0
            remaining = avg_time * (total_entities * 3 - processed_pairs)
            print(f"      â±ï¸ é¢„è®¡å‰©ä½™æ—¶é—´: {remaining / 60:.1f} åˆ†é’Ÿ")

        # ä¿å­˜æ‰¹æ¬¡æ—¶å¢åŠ è¯¦ç»†è¾“å‡º
        if len(results) >= batch_size:
            save_batch(output_file, results, batch_count)
            batch_count += 1
            results = []
            print(f"\nğŸ”¶ å·²å¤„ç† {processed_pairs} ä¸ªè¯­è¨€å¯¹")
            print(f"ğŸ•’ å¹³å‡é€Ÿåº¦: {processed_pairs / (time.time() - start_time):.1f} å¯¹/ç§’")

    # æœ€ç»ˆä¿å­˜å’Œç»Ÿè®¡
    if results:
        save_batch(output_file, results, batch_count)

    # æ–°å¢æœ€ç»ˆç»Ÿè®¡
    total_time = time.time() - start_time
    print("\nğŸ‰ å¤„ç†å®Œæˆï¼æœ€ç»ˆç»Ÿè®¡:")
    print(f"  æ€»è®¡å¤„ç†å®ä½“: {total_entities} ä¸ª")
    print(f"  å¤„ç†è¯­è¨€å¯¹: {processed_pairs} å¯¹")
    print(f"  å‘ç°åŒ¹é…é¡¹: {len(results)} æ¡")
    print(f"  ç”Ÿæˆæ‰¹æ¬¡æ–‡ä»¶: {batch_count + 1 if results else batch_count} ä¸ª")
    print(f"  æ€»è€—æ—¶: {total_time / 60:.1f} åˆ†é’Ÿ")
    print(f"  å¹³å‡é€Ÿåº¦: {processed_pairs / total_time:.1f} å¯¹/ç§’")


def save_batch(output_file: str, results: List, batch_num: int):
    filename = f"{output_file}_batch_{batch_num}.json"
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ ä¿å­˜æ‰¹æ¬¡ {batch_num} -> {os.path.abspath(filename)}")
    print(f"   æœ¬æ‰¹æ¬¡åŒ…å« {len(results)} æ¡å¯¹é½ç»“æœ")


if __name__ == "__main__":
    process_entities(
        input_file="",
        output_file="",
        batch_size=100
    )